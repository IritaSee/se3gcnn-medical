{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49a0180",
   "metadata": {},
   "source": [
    "# Training SE3GCNN on NIfTI Medical Data\n",
    "This notebook demonstrates how to train the SE3GCNN model on NIfTI medical imaging data. The model is designed to perform segmentation while being equivariant to rotations and translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2716e5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "First, let's import all the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05287543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from focal_loss import focal_loss\n",
    "\n",
    "# Import local modules\n",
    "import model_util\n",
    "import mesh_util\n",
    "from datagen import get_spatial_blocks\n",
    "from train_drivedata import run_steerable_gcnn, run_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b8ab1",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Define the training parameters and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['Schwannoma', 'Pituitary', 'Metastases', 'Meningioma', 'AVM']\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Data parameters\n",
    "        self.path = \"data/\"  # Path to your NIfTI data\n",
    "        self.grid_size = 7\n",
    "        self.num_classes = len(CLASSES)\n",
    "        self.num_shells = 1  # Single channel MRI data\n",
    "        \n",
    "        # Model parameters\n",
    "        self.interpolate = True\n",
    "        self.num_rays = 5\n",
    "        self.samples_per_ray = 2\n",
    "        self.ray_len = None  # Radius of the spherical kernel, None uses default arc length\n",
    "        self.watson_param = 10\n",
    "        self.model_capacity = \"small\"  # or \"big\"\n",
    "        \n",
    "        # Training parameters\n",
    "        self.b_size = 16\n",
    "        self.iter = 200\n",
    "        self.lr = 0.0001\n",
    "        self.alpha = 0.25  # Focal loss parameter\n",
    "        self.gamma = 2.0   # Focal loss parameter\n",
    "        self.cuda = 0      # GPU device index\n",
    "        self.train_split = 0.8  # 80% training, 20% validation\n",
    "        \n",
    "        # Other parameters\n",
    "        self.bias = True\n",
    "        self.lin_bias = True\n",
    "        self.spatial_bias = True\n",
    "        self.lin_bn = True\n",
    "        self.pooling = 'max'\n",
    "        self.exp_name = 'brain_tumor_classification'\n",
    "        self.run_path = 'results'\n",
    "        self.data_aug = True  # Enable data augmentation for medical images\n",
    "        self.spatial_kernel_size = (7, 7, 7)  # Correct - tuple of integers for 3D convolution\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43cdad",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "Create a custom Dataset class to load and preprocess NIfTI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621565a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import interpolate\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, base_dir, classes=CLASSES, transform=None, train=True, train_ratio=0.8, random_state=42):\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.train = train\n",
    "        \n",
    "        # Collect all file paths and labels\n",
    "        self.data = []\n",
    "        for class_idx, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(base_dir, class_name)\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.endswith('.nii') or file.endswith('.nii.gz'):\n",
    "                    self.data.append({\n",
    "                        'path': os.path.join(class_dir, file),\n",
    "                        'label': class_idx,\n",
    "                        'class': class_name\n",
    "                    })\n",
    "        \n",
    "        # Split into train/test\n",
    "        train_data, test_data = train_test_split(\n",
    "            self.data, \n",
    "            train_size=train_ratio,\n",
    "            random_state=random_state,\n",
    "            stratify=[d['label'] for d in self.data]\n",
    "        )\n",
    "        \n",
    "        self.data = train_data if train else test_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load NIfTI file\n",
    "        nifti_img = nib.load(item['path'])\n",
    "        image_data = nifti_img.get_fdata()\n",
    "        \n",
    "        # Preprocess data\n",
    "        processed_data = self.preprocess_data(image_data)\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return processed_data, label\n",
    "    \n",
    "    def preprocess_data(self, image_data):\n",
    "        # Normalize to [0, 1]\n",
    "        image_data = (image_data - image_data.min()) / (image_data.max() - image_data.min() + 1e-8)\n",
    "        \n",
    "        # Standardize\n",
    "        image_data = (image_data - image_data.mean()) / (image_data.std() + 1e-8)\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        image_tensor = torch.from_numpy(image_data).float()\n",
    "        \n",
    "        # Add channel dimension if needed\n",
    "        if len(image_tensor.shape) == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Resize to a standard size if needed (e.g., 128x128x128)\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "        \n",
    "        return image_tensor\n",
    "\n",
    "class Transform3D:\n",
    "    def __init__(self, output_size=(128, 128, 128), data_aug=False):\n",
    "        self.output_size = output_size\n",
    "        self.data_aug = data_aug\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Resize to standard size\n",
    "        x = interpolate(x.unsqueeze(0), size=self.output_size, mode='trilinear', align_corners=True).squeeze(0)\n",
    "        \n",
    "        if self.data_aug and torch.rand(1).item() > 0.5:\n",
    "            # Random rotation (90 degree increments)\n",
    "            k = torch.randint(4, (1,)).item()\n",
    "            x = torch.rot90(x, k, dims=[1, 2])\n",
    "            \n",
    "            # Random flips\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                x = x.flip(1)\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                x = x.flip(2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008abc1",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Training\n",
    "Set up the model, optimizer, and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_training():\n",
    "    # Initialize wandb with config\n",
    "    wandb.init(\n",
    "        project=args.exp_name,\n",
    "        config={\n",
    "            \"model_capacity\": args.model_capacity,\n",
    "            \"learning_rate\": args.lr,\n",
    "            \"batch_size\": args.b_size,\n",
    "            \"epochs\": args.iter,\n",
    "            \"data_augmentation\": args.data_aug,\n",
    "            \"num_classes\": args.num_classes\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Setup transform\n",
    "    transform = Transform3D(output_size=(128, 128, 128), data_aug=args.data_aug)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BrainTumorDataset(\n",
    "        base_dir=args.path,\n",
    "        transform=transform,\n",
    "        train=True,\n",
    "        train_ratio=args.train_split\n",
    "    )\n",
    "    \n",
    "    test_dataset = BrainTumorDataset(\n",
    "        base_dir=args.path,\n",
    "        transform=transform,\n",
    "        train=False,\n",
    "        train_ratio=args.train_split\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=args.b_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=args.b_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model and move to device\n",
    "    device = f\"cuda:{args.cuda}\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, wandb_name = run_steerable_gcnn(args, device, True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set up training\n",
    "    # Calculate class weights based on dataset distribution\n",
    "    class_counts = torch.bincount(torch.tensor([data['label'] for data in train_dataset.data]))\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "    criterion = focal_loss(\n",
    "        alpha=class_weights.to(device),\n",
    "        gamma=args.gamma,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.1,\n",
    "        patience=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return model, train_loader, test_loader, criterion, optimizer, scheduler, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd0af5",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "Run the training loop with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train():\n",
    "    model, train_loader, test_loader, criterion, optimizer, scheduler, device = initialize_training()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.run_path, exist_ok=True)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(args.iter):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}')):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                wandb.log({\n",
    "                    'train_batch_loss': loss.item(),\n",
    "                    'train_batch_acc': 100. * predicted.eq(targets).sum().item() / targets.size(0)\n",
    "                })\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss / len(train_loader),\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss / len(test_loader),\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, f'{args.run_path}/best_model.pth')\n",
    "            \n",
    "            # Create and log confusion matrix\n",
    "            cm = confusion_matrix(all_targets, all_preds)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=CLASSES,\n",
    "                       yticklabels=CLASSES)\n",
    "            plt.title(f'Confusion Matrix - Epoch {epoch+1}')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.tight_layout()\n",
    "            wandb.log({'confusion_matrix': wandb.Image(plt)})\n",
    "            plt.close()\n",
    "            \n",
    "            # Log classification report\n",
    "            report = classification_report(all_targets, all_preds, \n",
    "                                        target_names=CLASSES, \n",
    "                                        output_dict=True)\n",
    "            wandb.log({'classification_report': report})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911cf8a",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "Execute the training process and monitor with wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88affc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c279d16",
   "metadata": {},
   "source": [
    "## 7. Visualization and Evaluation\n",
    "After training, visualize results and evaluate model performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
