{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86a3b1c",
   "metadata": {},
   "source": [
    "# Training SE3GCNN on NIfTI Medical Data (No WandB)\n",
    "This notebook demonstrates how to train the SE3GCNN model on NIfTI medical imaging data without wandb dependency. The model is designed to perform brain tumor classification while being equivariant to rotations and translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4223ea5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "First, let's import all the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from focal_loss import focal_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import interpolate\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Import local modules\n",
    "import model_util\n",
    "import mesh_util\n",
    "from datagen import get_spatial_blocks\n",
    "from train_drivedata import run_steerable_gcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea8389",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Define the training parameters and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['Schwannoma', 'Pituitary', 'Metastases', 'Meningioma', 'AVM']\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Data parameters\n",
    "        self.path = \"data/\"  # Path to your NIfTI data\n",
    "        self.grid_size = 7\n",
    "        self.num_classes = len(CLASSES)\n",
    "        self.num_shells = 1  # Single channel MRI data\n",
    "        \n",
    "        # Model parameters\n",
    "        self.interpolate = True\n",
    "        self.num_rays = 5\n",
    "        self.samples_per_ray = 2\n",
    "        self.ray_len = None  # Radius of the spherical kernel, None uses default arc length\n",
    "        self.watson_param = 10\n",
    "        self.model_capacity = \"small\"  # or \"big\"\n",
    "        \n",
    "        # Training parameters\n",
    "        self.b_size = 16\n",
    "        self.iter = 200\n",
    "        self.lr = 0.0001\n",
    "        self.alpha = 0.25  # Focal loss parameter\n",
    "        self.gamma = 2.0   # Focal loss parameter\n",
    "        self.cuda = 0      # GPU device index\n",
    "        self.train_split = 0.8  # 80% training, 20% validation\n",
    "        \n",
    "        # Other parameters\n",
    "        self.bias = True\n",
    "        self.lin_bias = True\n",
    "        self.spatial_bias = True\n",
    "        self.lin_bn = True\n",
    "        self.pooling = 'max'\n",
    "        self.run_path = 'results'\n",
    "        self.data_aug = True  # Enable data augmentation for medical images\n",
    "        self.spatial_kernel_size = (7, 7, 7)  # Correct - tuple of integers for 3D convolution\n",
    "        \n",
    "        # Create timestamp for unique run folder\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.run_path = os.path.join(self.run_path, f\"run_{timestamp}\")\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f50bef",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "Create a custom Dataset class to load and preprocess NIfTI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d432d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, base_dir, classes=CLASSES, transform=None, train=True, train_ratio=0.8, random_state=42):\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.train = train\n",
    "        \n",
    "        # Collect all file paths and labels\n",
    "        self.data = []\n",
    "        for class_idx, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(base_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"Warning: Directory {class_dir} does not exist!\")\n",
    "                continue\n",
    "                \n",
    "            files = [f for f in os.listdir(class_dir) if f.endswith('.nii') or f.endswith('.nii.gz')]\n",
    "            print(f\"Found {len(files)} files in class {class_name}\")\n",
    "            \n",
    "            for file in files:\n",
    "                self.data.append({\n",
    "                    'path': os.path.join(class_dir, file),\n",
    "                    'label': class_idx,\n",
    "                    'class': class_name\n",
    "                })\n",
    "        \n",
    "        # Split into train/test\n",
    "        if len(self.data) > 0:\n",
    "            train_data, test_data = train_test_split(\n",
    "                self.data, \n",
    "                train_size=train_ratio,\n",
    "                random_state=random_state,\n",
    "                stratify=[d['label'] for d in self.data]\n",
    "            )\n",
    "            \n",
    "            self.data = train_data if train else test_data\n",
    "            \n",
    "            # Print class distribution\n",
    "            class_dist = {}\n",
    "            for d in self.data:\n",
    "                if d['class'] not in class_dist:\n",
    "                    class_dist[d['class']] = 0\n",
    "                class_dist[d['class']] += 1\n",
    "            \n",
    "            print(f\"{'Training' if train else 'Validation'} set class distribution:\")\n",
    "            for cls, count in class_dist.items():\n",
    "                print(f\"  {cls}: {count} images\")\n",
    "        else:\n",
    "            print(\"No data found! Please check the data directory.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load NIfTI file\n",
    "        nifti_img = nib.load(item['path'])\n",
    "        image_data = nifti_img.get_fdata()\n",
    "        \n",
    "        # Preprocess data\n",
    "        processed_data = self.preprocess_data(image_data)\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(item['label'], dtype=torch.long)\n",
    "        \n",
    "        return processed_data, label\n",
    "    \n",
    "    def preprocess_data(self, image_data):\n",
    "        # Normalize to [0, 1]\n",
    "        min_val = image_data.min()\n",
    "        max_val = image_data.max()\n",
    "        if max_val > min_val:  # Avoid division by zero\n",
    "            image_data = (image_data - min_val) / (max_val - min_val + 1e-8)\n",
    "        \n",
    "        # Standardize\n",
    "        mean_val = image_data.mean()\n",
    "        std_val = image_data.std()\n",
    "        if std_val > 0:  # Avoid division by zero\n",
    "            image_data = (image_data - mean_val) / (std_val + 1e-8)\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        image_tensor = torch.from_numpy(image_data).float()\n",
    "        \n",
    "        # Add channel dimension if needed\n",
    "        if len(image_tensor.shape) == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Apply transform if provided\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "        \n",
    "        return image_tensor\n",
    "\n",
    "class Transform3D:\n",
    "    def __init__(self, output_size=(128, 128, 128), data_aug=False):\n",
    "        self.output_size = output_size\n",
    "        self.data_aug = data_aug\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Resize to standard size\n",
    "        x = interpolate(x.unsqueeze(0), size=self.output_size, mode='trilinear', align_corners=True).squeeze(0)\n",
    "        \n",
    "        if self.data_aug and torch.rand(1).item() > 0.5:\n",
    "            # Random rotation (90 degree increments)\n",
    "            k = torch.randint(4, (1,)).item()\n",
    "            x = torch.rot90(x, k, dims=[1, 2])\n",
    "            \n",
    "            # Random flips\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                x = x.flip(1)\n",
    "            if torch.rand(1).item() > 0.5:\n",
    "                x = x.flip(2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa9179",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Training\n",
    "Set up the model, optimizer, and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df160ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_training():\n",
    "    # Create output directory\n",
    "    os.makedirs(args.run_path, exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    with open(os.path.join(args.run_path, 'config.txt'), 'w') as f:\n",
    "        for key, value in vars(args).items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    # Setup transform\n",
    "    transform = Transform3D(output_size=(128, 128, 128), data_aug=args.data_aug)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BrainTumorDataset(\n",
    "        base_dir=args.path,\n",
    "        transform=transform,\n",
    "        train=True,\n",
    "        train_ratio=args.train_split\n",
    "    )\n",
    "    \n",
    "    test_dataset = BrainTumorDataset(\n",
    "        base_dir=args.path,\n",
    "        transform=transform,\n",
    "        train=False,\n",
    "        train_ratio=args.train_split\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=args.b_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=args.b_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model and move to device\n",
    "    device = f\"cuda:{args.cuda}\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    model, model_name = run_steerable_gcnn(args, device, True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Set up training\n",
    "    # Calculate class weights based on dataset distribution\n",
    "    if len(train_dataset.data) > 0:\n",
    "        class_counts = torch.bincount(torch.tensor([data['label'] for data in train_dataset.data]))\n",
    "        class_weights = 1. / class_counts.float()\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "        print(f\"Class weights: {class_weights}\")\n",
    "    else:\n",
    "        class_weights = torch.ones(len(CLASSES)) / len(CLASSES)\n",
    "    \n",
    "    criterion = focal_loss(\n",
    "        alpha=class_weights.to(device),\n",
    "        gamma=args.gamma,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.1,\n",
    "        patience=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return model, train_loader, test_loader, criterion, optimizer, scheduler, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc5a33",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "Run the training loop with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ea51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model, train_loader, test_loader, criterion, optimizer, scheduler, device = initialize_training()\n",
    "    \n",
    "    # Lists to store metrics for plotting\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    epochs = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(args.iter):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        batch_times = []\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.iter}')):\n",
    "            batch_start = time.time()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            batch_end = time.time()\n",
    "            batch_times.append(batch_end - batch_start)\n",
    "            \n",
    "            # Print intermediate results every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, '\n",
    "                      f'Acc: {100.*predicted.eq(targets).sum().item()/targets.size(0):.2f}%, '\n",
    "                      f'Time: {batch_end-batch_start:.2f}s')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(test_loader, desc='Validation'):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / (train_total or 1)  # Avoid division by zero\n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        val_acc = 100. * val_correct / (val_total or 1)  # Avoid division by zero\n",
    "        \n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        epochs.append(epoch + 1)\n",
    "        \n",
    "        # Print epoch results\n",
    "        epoch_end = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{args.iter} completed in {(epoch_end - epoch_start)/60:.2f} minutes\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Avg batch time: {np.mean(batch_times):.4f}s\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        with open(os.path.join(args.run_path, 'metrics.csv'), 'a') as f:\n",
    "            if epoch == 0:\n",
    "                f.write('epoch,train_loss,train_acc,val_loss,val_acc\\n')\n",
    "            f.write(f'{epoch+1},{train_loss:.6f},{train_acc:.6f},{val_loss:.6f},{val_acc:.6f}\\n')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(args.run_path, 'best_model.pth'))\n",
    "            \n",
    "            # Create and save confusion matrix\n",
    "            cm = confusion_matrix(all_targets, all_preds)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=CLASSES,\n",
    "                       yticklabels=CLASSES)\n",
    "            plt.title(f'Confusion Matrix - Epoch {epoch+1} - Accuracy: {val_acc:.2f}%')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(args.run_path, f'confusion_matrix_epoch_{epoch+1}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Save classification report\n",
    "            report = classification_report(all_targets, all_preds, \n",
    "                                          target_names=CLASSES, \n",
    "                                          output_dict=False)\n",
    "            with open(os.path.join(args.run_path, f'classification_report_epoch_{epoch+1}.txt'), 'w') as f:\n",
    "                f.write(report)\n",
    "            \n",
    "            print(f\"New best model saved! Validation accuracy: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Plot and save learning curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(args.run_path, 'learning_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate and print total training time\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time/3600:.2f} hours\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Results saved to {args.run_path}\")\n",
    "    \n",
    "    # Return the trained model and metrics\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54cfa1",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "Execute the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3067db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950d4cf",
   "metadata": {},
   "source": [
    "## 7. Visualization and Evaluation\n",
    "After training, load the best model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model():\n",
    "    # Load configuration and datasets\n",
    "    transform = Transform3D(output_size=(128, 128, 128), data_aug=False)  # No augmentation for evaluation\n",
    "    \n",
    "    test_dataset = BrainTumorDataset(\n",
    "        base_dir=args.path,\n",
    "        transform=transform,\n",
    "        train=False,\n",
    "        train_ratio=args.train_split\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=args.b_size, \n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    device = f\"cuda:{args.cuda}\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, _ = run_steerable_gcnn(args, device, True)\n",
    "    \n",
    "    # Load best model weights\n",
    "    checkpoint = torch.load(os.path.join(args.run_path, 'best_model.pth'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate model\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    report = classification_report(all_targets, all_preds, target_names=CLASSES)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Evaluation Results (Epoch {checkpoint['epoch']+1}):\")\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=CLASSES,\n",
    "               yticklabels=CLASSES)\n",
    "    plt.title(f'Confusion Matrix (Final) - Accuracy: {accuracy*100:.2f}%')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(args.run_path, 'final_confusion_matrix.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, report, cm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
